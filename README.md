# Large Multi-Language Models for News Translation



* In this repo you may find examples __how to fine-tune Large Language Models__ (LLM) and apply them to the real task of __news translation__.
* Also in this repo we provide __news parser__, so you can easily parse any news web page you want (for example CNN, BBC news) and test how pre-trained LLM would __translate parsed real news__.

<img width="1136" alt="Снимок экрана 2023-12-18 в 14 48 37" src="https://github.com/anafisa/Text2Text-Transformer/assets/30799388/1350c467-6ca4-47ca-8077-c1cfa0118c6f">


# Models used

__1. Facebook: M2M100(1.2b parameters)__ - is a multilingual encoder-decoder (seq-to-seq) model primarily intended for translation tasks.

__2. Google: mT5()__ - mT5 is pretrained on the mC4 corpus, covering 101 languages:

Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.

